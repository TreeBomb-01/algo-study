# GPT-review: 캐시(LRU) — 코드 리뷰

## ✅ 요약
- 현재 구현은 **정답이 아닙니다.** (일부 케이스에서 우연히 맞을 수는 있지만, 전반적으로 LRU 규칙을 보장하지 못함)
- 가장 큰 문제는 **집합(set)만으로 LRU(가장 오래 안 쓴 것) 순서를 관리할 수 없다는 점**입니다.
- `miss` 시 `cache.clear()` 후 과거 도시들을 역으로 끼워 넣는 방식은 **LRU의 의도와 다르며** 다수 케이스에서 오답을 만듭니다.
- 또한 **hit일 때 ‘최근 사용’으로 갱신**이 전혀 이뤄지지 않습니다.

---

## 🔍 문제점 상세

### 1) LRU 순서가 보존되지 않음
```python
cache = set()
...
if cities_l[i] in cache:  # hit
    answer += 1
else:                      # miss
    answer += 5
    if len(cache) < cacheSize:
        cache.add(cities_l[i])
    else:
        cache.clear()
        j = 0
        while len(cache) != cacheSize:
            cache.add(cities_l[i - j])
            j += 1
```
- `set`은 **순서를 보장하지 않음** → LRU에서 필수적인 **가장 오래된 항목** 판별이 불가능.
- `clear()` 후 `i, i-1, i-2, ...` 순서로 **최근 항목들만** 다시 채우는 방식은, 실제 LRU의 **“가장 오래 사용하지 않은 항목 1개만 제거”** 규칙과 다름.
- `hit` 시에도 **해당 항목을 “가장 최근” 위치로 이동하는 처리**가 없음 → LRU 갱신 실패.

### 2) 시간 복잡도 & 안전성
- miss 때마다 `clear()` + `while` 루프 → **불필요한 O(cacheSize)** 비용.  
- `i-j`가 음수가 되는 엣지 케이스를 방지하는 별도 가드가 없음(입력에 따라 런타임 위험).

### 3) 엣지: cacheSize = 0
- 문제 정의상 캐시가 비활성인 경우 **모든 접근이 miss** → `5 * len(cities)`가 답.  
- 현재 구현도 결과적으로 맞게 돌아갈 수 있으나, **명시적으로 분기**하는 편이 안전/명확.

---

## 🧠 LRU의 핵심 동작 (정확한 정의)
- **hit(캐시에 있음)** → 실행시간 +1, **그 항목을 “가장 최근”으로 이동**.
- **miss(캐시에 없음)** → 실행시간 +5,  
  - 캐시에 빈자리가 있으면 **그대로 추가**  
  - 가득 찼으면 **가장 오래 안 쓴 항목 1개만 제거** 후 **추가**

이를 위해 **순서를 보존**하는 자료구조(예: `collections.OrderedDict`)를 사용하면 구현이 간단하고 정확합니다.

---

## 🧪 반례 힌트
다음과 같이 **hit 후 최근성 갱신이 중요한 시나리오**에서 현재 코드와 정답이 갈립니다.
```
cacheSize = 2
cities = ["A", "B", "A", "C", "A"]
# 정답 로직:
# A(miss) -> [A]
# B(miss) -> [A,B]
# A(hit, 최근으로 이동) -> [B,A]
# C(miss, B evict) -> [A,C]
# A(hit) -> [C,A]
# 총 시간 = 5+5+1+5+1 = 17
# 현재 코드: set + clear 로직으로는 이 순서를 재현 불가 → 오답 가능
```

---

## ✅ 권장 방향
- `OrderedDict`를 사용해 **O(1)에 최근성 갱신** 및 **가장 오래된 항목 제거**를 구현하세요.
- 캐시 크기가 0이면 곧바로 `5 * len(cities)`를 반환.

---

## 📌 정리
- 현재 풀이는 **LRU의 핵심(순서/최근성 갱신/단건 제거)**을 보장하지 않아 신뢰할 수 없습니다.
- **순서가 있는 컨테이너**(예: `OrderedDict`)로 교체하면, 코드가 **짧고 정확**하며 **성능도 최적**입니다.
